{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a44228da-f16a-4e39-be1c-06ec389e77b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset hate_speech already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:28<00:00, 28.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DataFrame into hate_speech1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from currency_symbols import CurrencySymbols\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Define the regex patterns\n",
    "emoji_pattern = (\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    ")\n",
    "accented_characters = u\"\\u00C0-\\u00FF\"  # Latin-1 Supplement (accented characters)\n",
    "special_symbols = '©®™±≠≤≥∞π∑§¶†•′″‰′′←→↑↓↔↕'\n",
    "\n",
    "# ISO codes for all currencies \n",
    "currencies = [\n",
    "    'AFN', 'EUR', 'ALL', 'DZD', 'USD', 'EUR', 'AOA', 'XCD', 'XCD', 'ARS', 'AMD', 'AWG', 'AUD', 'EUR', 'AZN',\n",
    "    'BSD', 'BHD', 'BDT', 'BBD', 'BYN', 'EUR', 'BZD', 'XOF', 'BMD', 'INR', 'BTN', 'BOB', 'BOV', 'USD', 'BAM',\n",
    "    'BWP', 'NOK', 'BRL', 'USD', 'BND', 'BGN', 'XOF', 'BIF', 'CVE', 'KHR', 'XAF', 'CAD', 'KYD', 'XAF', 'XAF',\n",
    "    'CLP', 'CLF', 'CNY', 'AUD', 'AUD', 'COP', 'COU', 'KMF', 'CDF', 'XAF', 'NZD', 'CRC', 'XOF', 'EUR', 'CUP',\n",
    "    'CUC', 'ANG', 'EUR', 'CZK', 'DKK', 'DJF', 'XCD', 'DOP', 'USD', 'EGP', 'SVC', 'USD', 'XAF', 'ERN', 'EUR',\n",
    "    'SZL', 'ETB', 'EUR', 'FKP', 'DKK', 'FJD', 'EUR', 'EUR', 'EUR', 'XPF', 'EUR', 'XAF', 'GMD', 'GEL', 'EUR',\n",
    "    'GHS', 'GIP', 'EUR', 'DKK', 'XCD', 'EUR', 'USD', 'GTQ', 'GBP', 'GNF', 'XOF', 'GYD', 'HTG', 'USD', 'AUD',\n",
    "    'EUR', 'HNL', 'HKD', 'HUF', 'ISK', 'INR', 'IDR', 'XDR', 'IRR', 'IQD', 'EUR', 'GBP', 'ILS', 'EUR', 'JMD',\n",
    "    'JPY', 'GBP', 'JOD', 'KZT', 'KES', 'AUD', 'KPW', 'KRW', 'KWD', 'KGS', 'LAK', 'EUR', 'LBP', 'LSL', 'ZAR',\n",
    "    'LRD', 'LYD', 'CHF', 'EUR', 'EUR', 'MOP', 'MKD', 'MGA', 'MWK', 'MYR', 'MVR', 'XOF', 'EUR', 'USD', 'EUR',\n",
    "    'MRU', 'MUR', 'EUR', 'XUA', 'MXN', 'MXV', 'USD', 'MDL', 'EUR', 'MNT', 'EUR', 'XCD', 'MAD', 'MZN', 'MMK',\n",
    "    'NAD', 'ZAR', 'AUD', 'NPR', 'EUR', 'XPF', 'NZD', 'NIO', 'XOF', 'NGN', 'NZD', 'AUD', 'USD', 'NOK', 'OMR',\n",
    "    'PKR', 'USD', 'PAB', 'USD', 'PGK', 'PYG', 'PEN', 'PHP', 'NZD', 'PLN', 'EUR', 'USD', 'QAR', 'EUR', 'RON',\n",
    "    'RUB', 'RWF', 'EUR', 'SHP', 'XCD', 'XCD', 'EUR', 'EUR', 'XCD', 'WST', 'EUR', 'STN', 'SAR', 'XOF', 'RSD',\n",
    "    'SCR', 'SLE', 'SGD', 'ANG', 'XSU', 'EUR', 'EUR', 'SBD', 'SOS', 'ZAR', 'SSP', 'EUR', 'LKR', 'SDG', 'SRD',\n",
    "    'NOK', 'SEK', 'CHF', 'CHE', 'CHW', 'SYP', 'TWD', 'TJS', 'TZS', 'THB', 'USD', 'XOF', 'NZD', 'TOP', 'TTD',\n",
    "    'TND', 'TRY', 'TMT', 'USD', 'AUD', 'UGX', 'UAH', 'AED', 'GBP', 'USD', 'USD', 'USN', 'UYU', 'UYI', 'UYW',\n",
    "    'UZS', 'VUV', 'VES', 'VED', 'VND', 'USD', 'USD', 'XPF', 'MAD', 'YER', 'ZMW', 'ZWL', 'ZWG', 'XBA', 'XBB',\n",
    "    'XBC', 'XBD', 'XTS', 'XXX', 'XAU', 'XPD', 'XPT', 'XAG'\n",
    "]\n",
    "\n",
    "currency_symbols = [CurrencySymbols.get_symbol(x) for x in currencies]\n",
    "currency_symbols = [x for x in currency_symbols if x and not x.isalpha()]  # Filter out None values and alphabetic characters\n",
    "\n",
    "pattern_string = (\n",
    "    r'[^a-zA-Z0-9\\s' +\n",
    "    ''.join([re.escape(x) for x in string.punctuation]) +\n",
    "    ''.join([re.escape(x) for x in currency_symbols]) +\n",
    "    special_symbols +\n",
    "    emoji_pattern +\n",
    "    accented_characters +\n",
    "    ']'\n",
    ")\n",
    "\n",
    "pattern = re.compile(pattern_string, re.UNICODE)\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    #df_cleaned = df[~df.apply(lambda row: row.astype(str).str.contains(pattern).any(), axis=1)]\n",
    "    #df_cleaned['tweet'] = df_cleaned['tweet'].str.replace('\"', ' ')\n",
    "    #df_cleaned['tweet'] = df_cleaned['tweet'].str.strip()\n",
    "    #df_cleaned['tweet'] = df_cleaned['tweet'].str.rstrip()\n",
    "    #escaped_chars_pattern = re.compile(r'[' + ''.join([re.escape(c) for c in ['\\\\', '\\t', '\\n']]) + ']')\n",
    "    #df_cleaned['tweet'] = df_cleaned['tweet'].str.replace(escaped_chars_pattern, ' ', regex=True)\n",
    "    #excessive_whitespace_pattern = re.compile(r'\\s{2,}')\n",
    "    #df_cleaned['tweet'] = df_cleaned['tweet'].str.replace(excessive_whitespace_pattern, ' ', regex=True)\n",
    "    #df_cleaned.drop(['tweet'], axis=1, inplace=True)\n",
    "\n",
    "    df_cleaned = df\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def process_and_clean_csv(input_path):\n",
    "    df = pd.read_csv(input_path, header=0, index_col=False, sep=',', encoding='utf-8', on_bad_lines='skip')\n",
    "    df_cleaned = clean_dataframe(df)\n",
    "    return df_cleaned\n",
    "\n",
    "def upload_file(bucket_name, source_file_path, destination_blob_name):\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    storage.blob._DEFAULT_CHUNKSIZE = 100 * 1024 * 1024  # 100 MB chunk size\n",
    "    storage.blob._MAX_MULTIPART_SIZE = 100 * 1024 * 1024  # 100 MB max multipart size\n",
    "    blob.upload_from_filename(source_file_path, timeout=600)\n",
    "    print(f'{source_file_path} uploaded to {bucket_name} as {destination_blob_name}.')\n",
    "\n",
    "def load_to_bigquery(df, table_ref):\n",
    "    job = bigquery_client.load_table_from_dataframe(df, table_ref)\n",
    "    job.result()\n",
    "    print(f'Loaded DataFrame into {table_ref.table_id}.')\n",
    "\n",
    "# Service account key\n",
    "key_path = '/Users/chkapsalis/Downloads/nlp-project-427710-3e1a48df3dba.json'\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "# Google Cloud project id and dataset information\n",
    "project_id = 'nlp-project-427710'\n",
    "dataset_id = 'hate_speech'  # Replace with your dataset ID\n",
    "table_id = 'hate_speech1'  # Replace with your table ID\n",
    "\n",
    "# Initialization of the BigQuery client\n",
    "bigquery_client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "storage_client = storage.Client(project=project_id, credentials=credentials)\n",
    "\n",
    "# Create the dataset if it does not exist\n",
    "dataset_ref = bigquery_client.dataset(dataset_id)\n",
    "dataset = bigquery.Dataset(dataset_ref)\n",
    "\n",
    "try:\n",
    "    bigquery_client.get_dataset(dataset_ref)  # Make an API request.\n",
    "    print(f\"Dataset {dataset_id} already exists.\")\n",
    "except:\n",
    "    dataset = bigquery_client.create_dataset(dataset)  # Make an API request.\n",
    "    print(f\"Dataset {dataset_id} created.\")\n",
    "\n",
    "# Define the job configuration for loading data\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[\n",
    "        bigquery.SchemaField('count','INTEGER'),\n",
    "        bigquery.SchemaField('hate_speech_count','INTEGER'),\n",
    "        bigquery.SchemaField('offensive_language_count','INTEGER'),\n",
    "        bigquery.SchemaField('neither_count','INTEGER'),\n",
    "        bigquery.SchemaField('class','INTEGER'),\n",
    "        bigquery.SchemaField('tweet','STRING')\n",
    "    ],\n",
    "    \n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,\n",
    "    field_delimiter='|',  # used this custom delimiter to help make sense out of the data\n",
    "    autodetect=False,  # Automatically detect the schema\n",
    "    max_bad_records=2000,  # Allow up to 2000 bad records\n",
    "    ignore_unknown_values=True  # Ignore unknown values\n",
    ")\n",
    "\n",
    "source_folder = '/Users/chkapsalis/Downloads/hate_speech'\n",
    "cleaned_folder = '/Users/chkapsalis/Downloads/cleaned_hate_speech'\n",
    "\n",
    "os.makedirs(cleaned_folder, exist_ok=True)\n",
    "\n",
    "for filename in tqdm(os.listdir(source_folder)):\n",
    "    if filename.endswith('.csv'):\n",
    "        source_file = os.path.join(source_folder, filename)\n",
    "        df_cleaned = process_and_clean_csv(source_file)\n",
    "        # making sure that the dtypes of columns match what asserted in the schema\n",
    "        df_cleaned['count'] = pd.to_numeric(df_cleaned['count'], errors='coerce')\n",
    "        df_cleaned['hate_speech_count'] = pd.to_numeric(df_cleaned['hate_speech_count'], errors='coerce')\n",
    "        df_cleaned['offensive_language_count'] = pd.to_numeric(df_cleaned['offensive_language_count'], errors='coerce')\n",
    "        df_cleaned['neither_count'] = pd.to_numeric(df_cleaned['neither_count'], errors='coerce')\n",
    "        df_cleaned['class'] = pd.to_numeric(df_cleaned['class'], errors='coerce')\n",
    "\n",
    "        \n",
    "        # Upload the cleaned DataFrame to BigQuery\n",
    "        #load_to_bigquery(df_cleaned, dataset_ref.table(table_id))\n",
    "        load_to_bigquery(df_cleaned, dataset_ref.table(table_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1c02f-7fd4-486f-837f-58622d5b3e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d706ef-f9b8-4ae5-b10b-8f692313c243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd4c255-dd2f-47e8-842c-0038bfe7715d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba71f1-e013-4c17-b704-4492582955db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee54e30-b9da-47ac-bec5-f126a8a34217",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
